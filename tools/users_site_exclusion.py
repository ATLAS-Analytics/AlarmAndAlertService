# Deduplicate PerfSONAR data in all the indices where _id is not autogenerated
#
# it accepts a date range.
import sys
import json
from elasticsearch import Elasticsearch, exceptions as es_exceptions
from elasticsearch.helpers import scan, bulk
from datetime import datetime

with open('/config/config.json') as json_data:
    config = json.load(json_data,)

es = Elasticsearch(
    hosts=[{'host': config['ES_HOST'], 'scheme':'https'}],
    http_auth=(config['ES_USER'], config['ES_PASS']),
    timeout=60)

if es.ping():
    print('connected to ES.')
else:
    print('no connection to ES.')
    sys.exit(1)

start_int = '2020-10-01 00:00'
stop_int = '2020-10-02 00:00'
start_dt = datetime.strptime(start_int, '%Y-%m-%d %H:%M')
stop_dt = datetime.strptime(stop_int, '%Y-%m-%d %H:%M')
print('start:', start_dt, '\nstop: ', stop_dt)

# ### define what are the indices to deduplicate

indices = 'task_parameters*'

query = {
    "_source": ['taskparams.excludedSite', 'taskparams.userName'],
    "query": {
        "range": {
            "creationdate": {
                "gt": int(start_dt.timestamp()*1000),
                "lte": int(stop_dt.timestamp()*1000)
            }
        }
    }
}

docs_read = 0
scroll = scan(client=es, index=indices, query=query, timeout="5m")
for res in scroll:
    docs_read += 1
    if not docs_read % 500:
        print('docs read', docs_read)
    # if res['_id'] in ids:
        # ids[res['_id']].append(res['_index'])
    # else:
        # ids[res['_id']] = [res['_index']]

print('read:', docs_read)
